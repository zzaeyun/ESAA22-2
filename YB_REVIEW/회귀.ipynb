{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNk8SFF6Uwc+tRbVgh9Bqhx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA22-2/blob/main/YB_REVIEW/%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 회귀\n",
        "\n",
        "###**회귀 소개**\n",
        "\n",
        "**회귀(regression)**\n",
        "- 데이터 값이 평균과 같은 일정한 값으로 돌아가려는 경향을 이용한 통계학 기법\n",
        "- 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 통칭\n",
        "- 피처와 결정 값 데이터 기반에서 학습을 통해 **최적의 회귀 계수** 찾기 →머신러닝 회귀 예측의 핵심\n",
        "\n",
        "**회귀 유형**\n",
        "- **회귀 계수의 선형/비선형 여부** → 선형 회귀/비선형 회귀\n",
        "- 독립변수의 개수 → 단일 회귀/다중 회귀\n",
        "- 종속변수의 개수\n",
        "\n",
        "**분류와 회귀**\n",
        "- 지도학습의 두 가지 유형: 분류, 회귀\n",
        "- 분류: 예측값이 카테고리와 같은 이산형 클래스 값\n",
        "- 회귀: 예측값이 연속형 숫자 값\n",
        "\n",
        "**대표적인 회귀 모델**\n",
        "- 일반 선형 회귀: RSS(Residual Sum of Squares)를 최소화할 수 있도록 회귀 계수를 최적화, 규제(Regularization)를 적용하지 않음\n",
        "- 릿지(Ridge): L2 규제\n",
        "- 라쏘(Lasso): L1 규제\n",
        "- 엘라스틱넷(ElasticNet): L2, L1 규제를 함께 결합\n",
        "- 로지스틱 회귀(Logistic Regression): 분류에 사용되는 선형 모델\n",
        "\n",
        "\n",
        "###**단순 선형 회귀를 통한 회귀 이해**\n",
        "\n",
        "**단순 선형 회귀**\n",
        "- 독립변수도 하나, 종속변수도 하나\n",
        "- 선형의 관계로 표현\n",
        "\n",
        "**잔차**\n",
        "- 실제 값과 회귀 모델의 차이에 따른 오류 값\n",
        "- 최적의 회귀 모델을 만든다는 것은 잔차 합이 최소가 되는 모델\n",
        "- 오류는 +나 -가 될 수 있기 때문에 다음과 같은 방법으로 오류 합을 구함\n",
        "  - MAE(Mean Absolute Error): 절댓값을 취해서 더함\n",
        "  - RSS(Residual Sum of Square): 오류 값의 제곱을 구해서 더하는 방식\n",
        "  → 비용 함수, 손실 함수(loss function)\n",
        "\n",
        "###**비용 최소화하기 - 경사 하강법(Gradient Descent)소개**\n",
        "\n",
        "**경사 하강법**\n",
        "- 고차원 방정식에 대한 문제를 해결해 주면서 비용 함수 RSS를 최소화하는 방법을 직관적으로 제공하는 방식\n",
        "- '점진적인 하강' → 점진적으로 반복적인 계산을 통해 W 파라미터 값을 업데이트하면서 오류 값이 최소가 되는 W 파라미터를 구하는 방식\n",
        "- 최초 W에서부터 미분을 적용한 뒤 이 미분 값이 계속 감소하는 방향으로 순차적으로 W를 업데이트 → 깅루기가 감소하지 않는 지점을 비용 함수가 최소인 지점으로 간주\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "31NqXuE02CRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**다항 회귀와 과(대)적합/과소적합 이해**\n",
        "\n",
        "**다항 회귀 이해**\n",
        "- 독립변수(feature)와 종속변수(target)의 관계가 일차 방정식 형태인 직선으로만 표현될 수 없다 → 2차, 3차 방정식과 같은 다항식으로 표현되는 것을 다항(Polynomial) 회귀\n",
        "- **다항 회귀는 선형 회귀** \n",
        "  - 회귀에서 선형/비선형을 나누는 기준은 회귀 계수가 선형/비선형인지에 따르는 것\n",
        "- 단순 선형 회귀 직선형으로 표현한 것보다 다항 회귀 곡선형으로 표현한 것이 더 예측 성능이 높음\n",
        "- 사이킷런에서는 다항 회귀를 위한 클래스를 명시적으로 제공하지 않으므로 비선형 함수를 선형 모델에 적용시키는 방법을 사용해 구현\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4ZsZwopQ6RFs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZAea8qz0rsf",
        "outputId": "b903aafa-585e-4fdf-89b0-d5aa65c01391"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일차 단항식 계수 피처:\n",
            " [[0 1]\n",
            " [2 3]]\n",
            "변환된 2차 다항식 계수 피처:\n",
            " [[1. 0. 1. 0. 0. 1.]\n",
            " [1. 2. 3. 4. 6. 9.]]\n"
          ]
        }
      ],
      "source": [
        "#다항 회귀 구현\n",
        "#PolynomialFeatures를 이용하여 피처를 Polynomial 다항식 피처로 변환\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "import numpy as np\n",
        "\n",
        "X=np.arange(4).reshape(2,2)\n",
        "print('일차 단항식 계수 피처:\\n', X)\n",
        "\n",
        "poly=PolynomialFeatures(degree=2)\n",
        "poly.fit(X)\n",
        "poly_ftr=poly.transform(X)\n",
        "print('변환된 2차 다항식 계수 피처:\\n',poly_ftr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PolynomialFeatures(degree=2)**\n",
        "\n",
        "**단항 계수 피처 [x1, x2]**\n",
        "\n",
        "↓↓\n",
        "\n",
        "**다항 계수 피처 [1, x1, x2, x1^2, x1*x2, x2^2]**\n"
      ],
      "metadata": {
        "id": "WX0M616y-OSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_func(X):\n",
        "  y=1+2*X[:,0]+3*X[:,0]**2+4*X[:,1]**3\n",
        "  return y\n",
        "\n",
        "X=np.arange(4).reshape(2,2)\n",
        "print('일차 단항식 계수 feature:\\n', X)\n",
        "y=polynomial_func(X)\n",
        "print('삼차 다항식 결정값:\\n', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYzEL6ty-qRD",
        "outputId": "2d64efb2-8797-493e-a462-ae71f6c11129"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "일차 단항식 계수 feature:\n",
            " [[0 1]\n",
            " [2 3]]\n",
            "삼차 다항식 결정값:\n",
            " [  5 125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ftr=PolynomialFeatures(degree=3).fit_transform(X)\n",
        "print('3차 다항식 계수 feature: \\n', poly_ftr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeWGPoEx_njL",
        "outputId": "21fee210-7630-4b81-afce-f34c7f24728e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3차 다항식 계수 feature: \n",
            " [[ 1.  0.  1.  0.  0.  1.  0.  0.  0.  1.]\n",
            " [ 1.  2.  3.  4.  6.  9.  8. 12. 18. 27.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PolynomialFeatures(degree=3)**\n",
        "\n",
        "**단항 계수 피처 [x1, x2]**\n",
        "\n",
        "↓↓\n",
        "\n",
        "**3차 다항 계수 [1, x1, x2, x1^2, x1*x2, x2^2, x1^3, x1^2*x2, x1*x2^2, x2^3]**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uj6dKZF-_YC7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**다항 회귀를 이용한 과소적합 및 과적합 이해**\n",
        "- 다항 회귀는 피처의 직선적 관계가 아닌 복잡한 다항 관계를 모델링\n",
        "- → 다항식의 차수가 높아질수록 매우 복잡한 피처 간의 관계까지 모델링\n",
        "- → 다항 회귀의 차수(degree)를 높일수록 학습 데이터에만 너무 맞춘 학습이 이루어져 테스트 데이터에서는 오히려 예측 정확도가 떨어짐\n",
        "- **차수가 높아질수록 과적합 문제가 크게 발생**\n",
        "\n",
        "**편향-분산 트레이드오프(Bias-Variance Trade off)**\n",
        "- 머신러닝이 극복해야 할 가장 중요한 이슈\n",
        "- 편향과 분산은 한 쪽이 높으면 한 쪽이 낮아지는 경향이 존재\n",
        "  - 편향이 높으면 분산은 낮아짐(과소적합)\n",
        "  - 분산이 높으면 편향이 낮아짐(과적합)\n",
        "- **편향과 분산이 서로 트레이드오프를 이루면서 오류 Cost 값이 최대로 낮아지는 모델을 구축하는 것이 가장 효율적인 머신러닝 예측 모델**\n",
        "\n"
      ],
      "metadata": {
        "id": "pXj6uFzI_6I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷**\n",
        "\n",
        "**규제 선형 모델의 개요**\n",
        "- 좋은 머신러닝 회귀 모델 → 적절히 데이터에 적합하면서도 회귀 계수가 기하급수적으로 커지는 것을 제어할 수 있어야 함\n",
        "- 기존 선형 모델의 비용 함수는 RSS를 최소화하는 것만 고려 → 학습 데이터에 지나치게 맞추고 회귀 계수가 쉽게 커짐\n",
        "- 이를 반영하여 비용 함수는 학습 데이터의 잔차 오류 값을 최소로 하는 RSS 최소화 방법과 과적합을 방지하기 위해 회귀 계수 값이 커지지 않도록 하는 방법이 균형을 이루어야 함\n",
        "\n",
        "**규제(Regularization)**\n",
        "- 비용 함수에 alpha 값으로 페널티를 부여해 회귀 계수 값의 크기를 감소시켜 과적합을 개선하는 방식\n",
        "- L2: alpha*\\W\\2 와 같이 W의 제곱에 대해 페널티를 부여하는 방식\n",
        "- L1: alpha*\\W\\1와 같이 W의 절댓값에 대해 페널티를 부여\n",
        "  - L1 규제를 적용하면 영향력이 크지 않은 회귀 계수 값을 0으로 변환"
      ],
      "metadata": {
        "id": "mt0GqYR2AwSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**릿지 회귀**\n",
        "- W의 제곱에 대해 페널티를 부여하는 L2 규제를 선형 회귀에 적용한 것\n",
        "- 사이킷런의 Ridge 클래스\n",
        "- 주요 생성 파라미터는 alpha, alpha L2 규제 계수에 해당\n",
        "- 회귀 계수를 0으로 만들지는 않음\n",
        "\n",
        "**라쏘 회귀**\n",
        "- W의 절대값에 페널티를 부여하는 L1 규제를 선형 회귀에 적용한 것\n",
        "- 불필요한 회귀 계수를 급격하게 감소시켜 0으로 만들고 제거\n",
        "   - → 적절한 피처만 회귀에 포함시키는 피처 선택의 특성\n",
        "- 사이킷런의 Lasso 클래스\n",
        "- 주요 생성 파라미터는 alpha, alpha L1 규제 계수에 해당\n",
        "\n",
        "**엘라스틱넷 회귀**\n",
        "- L2 규제와 L2 규제를 결합한 회귀\n",
        "- L1 규제 → 서로 상관관계가 높은 피처들의 경우에 중요 피처만을 셀렉션하고 다른 피처들은 모두 회귀 계수를 0으로 만드는 성향\n",
        "- alhpa 값에 따라 회귀 계수의 값이 급격히 변동할 수 있는데 이를 완화하기 위해 L2 규제를 라쏘에 추가한 것 \n",
        "- 사이킷럼의 ElasticNet 클래스\n",
        "- 주요 생성 파라미터는 alpha와 l1_ratio\n",
        "  - a * L1 + b* L2 (a는 L1 규제의 alpha 값, b는 L2 규제의 alpha 값)\n",
        "  - **alpha: a + b**\n",
        "  - **l1_ration: a / (a + b)**\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**선형 회귀 모델을 위한 데이터 변환**\n",
        "- 선형 회귀 모델과 같은 선형 모델은 일반적으로 피처와 타깃값 간에 선형의 관계가 있다고 가정하고, 최적의 선형 함수를 찾아내 결과값을 예측\n",
        "- → 타깃값, 피처값의 경우 정규 분포 형태가 아니라 특정값의 분포가 치우친 왜곡(Skew)된 형태의 분포도일 경우 예측 성능에 부정적인 영향을 미칠 간능성이 높음 \n",
        "- 선형 회귀 모델을 적용하기 전에 데이터에 대한 **스케일링/정규화 작업**을 수행하는 것이 일반적\n",
        "\n",
        "**사이킷런을 이용해 피처 데이터 세트에 적용하는 변환 작업**\n",
        "1. 스케일링/정규화\n",
        "- StandardScaler 클래스를 이용해 평균이 0, 분산이 1인 표준 정규 분포를 가진 데이터 세트로 변환\n",
        "- MinMaxScaler 클래스를 이용해 최솟값이 0이고 최댓값이 1인 값으로 정규화를 수행\n",
        "\n",
        "2. 다항 특성을 적용하여 변환\n",
        "- 스케일링/정규화를 수행한 후 예측 성능에 향상이 없을 경우\n",
        "- 피처의 개수가 많을 경우 다항 변환으로 생성되는 피처의 개수가 기하급수로 늘어나 과적합의 이슈 발생 가능 \n",
        "\n",
        "3. **로그 변환**\n",
        "- 원래 값에 log 함수를 적용하면 보다 정규 분포에 가까운 형태로 값이 분포\n",
        "- 로그 변환은 매우 유용한 변환\n",
        "- 실제로 선형 회귀에서는 로그 변환을 훨씬 많이 활용\n",
        "\n",
        "**타깃 값의 경우 일반적으로 로그 변환을 적용**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8F1G8cVECxdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**로지스틱 회귀**\n",
        "\n",
        "- 선형 회귀 방식을 분류에 적용한 알고리즘 → 분류에 사용\n",
        "- 선형 함수의 회귀 최적선을 찾는 것이 아니라 시그모이드(Sigmoid) 함수 최적선을 찾고 시그모이드 함수의 반환 값을 확률로 간주해 확률에 따라 분류를 결정\n",
        "- 자연/사회 현상에서 특정 변수의 확률 값은 시그모이드 함수와 같은 S자 커브 형태\n",
        " - 시그모이드 함수: y=1/(1+exp(-x))\n",
        " - x 값의 범위에 상관없이 y 값이 항상 0과 1사이 값을 반환\n",
        "- 가볍고 빠르지만, 이진 분류 예측 성능도 뛰어남\n",
        "\n",
        "\n",
        "**회귀 트리**\n",
        "- 비선형 회귀 함수를 통해 결괏값을 예측하지만 회귀 계수의 결합이 비선형\n",
        "- 트리 기반의 회귀는 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측\n",
        "- 회귀 예측값은 리프 노드(터미널 노드)에 속한 데이터 값의 평균값을 구해 회귀 예측값을 계산\n"
      ],
      "metadata": {
        "id": "tMGqGt0dGHHa"
      }
    }
  ]
}