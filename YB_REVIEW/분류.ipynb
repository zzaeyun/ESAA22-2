{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvBR6tNxCaudt5baT7Jxop",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA22-2/blob/main/YB_REVIEW/%EB%B6%84%EB%A5%98.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **> 분류**\n"
      ],
      "metadata": {
        "id": "UAdKnw3XPEeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**개요**\n",
        "\n",
        "- 지도 학습이란?\n",
        "\n",
        "레이블(Label)이라는 정답이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식\n",
        "\n",
        "- 분류(Classification)\n",
        "1. 학습 데이터로 주어진 데이터의 피처와 레이블값을 머신러닝 알고리즘으로 학습해 모델을 생성\n",
        "2. 모델에 새로운 데이터 값이 주어졌을 때 미지의 레이블 값을 예측\n",
        "\n",
        "분류를 구현하는 머신러닝 알고리즘\n",
        "- Naive Bayes: 베이즈 통꼐와 생성 모델에 기반한 나이브 베이즈\n",
        "- Logistic Regression: 독립변수와 종속변수의 선형 관계성에 기반한 로지스틱 회귀\n",
        "- Decision Tree: 데이터 균일도에 따른 규칙 기반의 결정 트리\n",
        "- Support Vector Machine: 개별 클래스 간의 최대 분류 마진을 효과적으로 찾아주는 서포트 벡터 머신\n",
        "- Nearest Neighbor: 근접 거리를 기준으로 하는 최소 근접 알고리즘\n",
        "- Neural Network: 심층 연결 기반의 신경망\n",
        "- Ensemble: 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블\n",
        "  - 앙상블은 분류에서 가장 각광을 받는 방법 중 하나\n",
        "  - 정형 데이터의 예측 분석 영역에서는 앙상블이 매우 높은 예측 성능을 보여줌\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dyfUB7D8Eid_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**앙상블**\n",
        "\n",
        "1. 배깅(Bagging)\n",
        "- 랜덤 포레스트 (Random Forest): 뛰어난 예측 성능, 상대적으로 빠른 수행 시간, 유연성 등의 장점\n",
        "2. 부스팅(Boosting)\n",
        "- 그래디언트 부스팅(Gradient Boosting): 뛰어난 예측 성능을 가지지만 수행 시간이 너무 오래 걸리는 단점으로 최적화 모델 튜닝의 어려움 존재\n",
        "- XgBoost(eXtra Gradient Boost), Light GBM: 예측 성능을 한단계 발전시키면서 수행 시간을 단축\n",
        "3. 스태킹(Stacking)\n",
        "\n",
        "\n",
        "- 앙상블은 대부분 동일한 알고리즘을 결합\n",
        "- 결정 트리는 쉽고 유연한 알고리즘이지만 과적합 문제가 발생, 이는 앙상블 기법에서 장점으로 사용\n",
        "\n"
      ],
      "metadata": {
        "id": "RCn18QCVPI9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**결정 트리**\n",
        "\n",
        "결정 트리(Decision Tree)\n",
        "- 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것\n",
        "- 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우함\n",
        "\n",
        "결정 트리의 구조\n",
        "- 규칙 노드(Decistio): 규칙의 조건\n",
        "- 리프 노드(Leaf Node): 결정된 클래스 값\n",
        "\n",
        "- 많은 규칙이 있다는 것은 분류를 결정하는 방식이 복잡하다는 것으로 과적합을 이어질 수 있음\n",
        "- 트리의 깊이(depth)가 깊어질수록 결정 트리의 예측 성능이 저하될 가능성이 높음\n",
        "- 트리를 분한할 때 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요\n",
        "  - 결정 노드는 정보 균일도가 높은 데이터 세트를 먼저 선택할 수 있도록 규칙 조건을 만듦 -> 가장 효율적인 분류 방식\n",
        "  - 정보 이득(information Gain)지수와 지니 계수로 균일도를 측정\n",
        "\n",
        "! 정보 이득: 1-엔트로피, 엔트로피는 주어진 데이터 집합의 혼잡도를 의미, 서로 다른 값이 섞여 있으면 엔트로피가 높고, 같은 값이 섞여 있으면 엔트로피가 낮음\n",
        "\n",
        "! 지니 계수: 경제학에서 줄평등 지수를 나타낼 때 사용하는 계수, 0이 가장 평등하고 1로 갈수록 불평등, 머신러닝에서는 지니 계수가 낮수록 데이터 균일도가 높은 것으로 해석하여 지니 계수가 낮은 속성을 기준으로 분할\n",
        "\n",
        "결정 트리의 특징\n",
        "- 장점\n",
        "1. 알고리즘이 쉽고 직관적임: 룰이 매우 명확, 시각화 가능\n",
        "2. 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않음\n",
        "- 단점\n",
        "1. 과적합으로 정확도가 떨어진다는 단점: 트리의 크기를 사전에 제한하는 것이 성능 튜닝에 더 도움됨\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wCm-8HC4JFN2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYbYhxIKD3Pu"
      },
      "outputs": [],
      "source": [
        "#결정 트리 \n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_clf=DecisionTreeClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **앙상블 학습**\n",
        "\n",
        "앙상블 학습(Ensemble Learning)\n",
        "- 앙상블 학습을 통한 분류는 여러 개의 분류기를 생성하고 그 예측을 결합하여 정확한 최종 예측을 도출\n",
        "- 쉽고 편하면서도 강력한 성능을 보유하고 있음\n",
        "- XGBoost, LightGBM, Stacking 등\n",
        "\n",
        "앙상블 종류\n",
        "1. 보팅(Voting)\n",
        "- 선형 회귀, K 최근접 이웃, 서포트 벡터 머신이라는 3개의 ML 알고리즘이 같은 데이터 세트에 대해 학습하고 예측한 결과를 가지고 보팅을 통해 최종 예측 결과 선적\n",
        "2. 배깅(Bagging)\n",
        "- 개별 분류기에 할당된 학습 데이터는 원본 학습 데이터를 샘플링해 추출(부트스트래핑)하고 이에 대해 학습을 통해 개별적인 예측을 수행한 결과를 보팅\n",
        "3. 부스팅(Boosting)\n",
        "- 여러 개의 분류기가 순차적으로 학습을 수행하면서 앞에서 학습한 분류기가 예측이 틀린 데이터에 대해서는 올바르게 예측하도록 가중치를 부여하면서 학습과 예측을 진행하는 것"
      ],
      "metadata": {
        "id": "nL7zpS-uMm6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**랜덤 포레스트**\n",
        "\n",
        "랜덤 포레스트: 배깅의 대표적인 알고리즘\n",
        "- 배깅: 같은 알고리즘으로 여러 개의 분류기를 만들어서 보팅으로 최종 결정하는 알고리즘\n",
        "\n",
        "랜덤 포레스트 개요\n",
        "- 비교적 빠른 수행 속도, 다양한 영역에서 높은 예측 성능\n",
        "- 여러 개의 결정 트리 분류기가 전체 데이터에서 배깅 방식으로 각자의 데이터를 샘플링해 개별적으로 학습을 수행한 뒤 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 하게 됨\n",
        "\n",
        "- 부트스트래핑으로 만들어진 서브세트의 데이터 건수는 전체 데이터 건수와 동일하지만, 개별 데이터가 중첩되어 만들어짐\n",
        "  - 하이퍼 파라미터 n_estimator로 서브세트 개수 조절\n",
        "  - 개별 트리가 학습하는 데이터 세트는 전체 데이터에서 일부가 중첩되게 샘플링된 데이터 세트: 부트스트래핑(bootstrapping)\n",
        "\n",
        "랜덤 포레스트 하이퍼 파라미터 및 튜닝\n",
        "- 하이퍼 파라미터가 많고 튜닝에 많은 시간이 소요\n",
        "1. n_estimators\n",
        "2. max_features\n",
        "3. max_depth\n"
      ],
      "metadata": {
        "id": "bVWxDgYyPA-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_clf=RandomForestClassifier()"
      ],
      "metadata": {
        "id": "EtL2k776Mmj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**GBM(Gradient Boosting Machine)**\n",
        "\n",
        "부스팅 알고리즘\n",
        "- 부스팅 알고리즘은 여러 개의 약한 학습기를 순차적으로 학습-예측하면서 잘못 예측한 데이터에 가중치 부여를 통해 오류를 개선해 나가면서 학습하는 방식\n",
        "- AdaBoost(Adaptive boosting), 그래디언트 부스팅\n",
        "\n",
        "GBM의 개요\n",
        "- 가중치 업데이트를 경사 하강법(Gradient Descent)를 이용\n",
        "- 오류 값: 실제 값 - 예측값\n",
        "- CART 기반의 다른 알고리즘과 마찬가지로 분류 및 회귀도 가능\n",
        "- GBM이 랜덤포레스트보다는 예측 성능이 조금 뛰어나지만 수행 시간이 오래걸리고 하이퍼 파라미터 튜닝 노력도 필요\n",
        "\n",
        "GBM 하이퍼 파라미터 및 튜닝\n",
        "- 트리 기반 자체 파라미터: n_estimators, max_depth, max_features는 동일하게 사용\n",
        "- loss\n",
        "- learning_rate: GBM이 학습을 진행할 때마다 적용하는 학습률\n",
        "- n_estimators: weak learner의 개수\n",
        "- subsample: 데이터의 샘플링 비율\n"
      ],
      "metadata": {
        "id": "gHh36l5ERJjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "gb_clf=GradientBoostingClassifier()"
      ],
      "metadata": {
        "id": "JyhDj_HUR4KO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**XGBoost(eXtra Gradient Boost)**\n",
        "\n",
        "XGBoost의 개요\n",
        "- 분류에 있어서 다른 머신러닝보다 뛰어난 예측 성능\n",
        "- GBM에 기반하지만 GBM의 단점인 느린 수행 시간 및 과적합 규제 부재 등의 문제를 해결\n",
        "\n",
        "사이킷런 래퍼 XGBoost의 개요\n",
        "- 사이킷런의 다른 유틸리티를 그대로 사용하고 fit, predict만으로 학습과 예측이 가능하도록 만든 XGBoost 래퍼 클래스\n",
        "\n",
        "사이킷런 래퍼 XGBoost의 하이퍼 파라미터\n",
        "- 사이킷런에서 일반적으로 사용하는 하이퍼 파라미터와 호환성을 유지하기 위해 이름을 변경\n",
        "- learning_rate: 학습률, 0에서 1사이의 값을 지정, 부스팅 스텝을 반복적으로 사용할 때 업데이트되는 학습률 값, 디폴트는 0.1, 0.01~0.2 사이의 값을 선호\n",
        "- subsample: 트리가 커져서 과적합되는 것을 제어하기 위해 데이터를 샘플링하는 비율을 지정\n",
        "- reg_lambda: L2 Regularization 적용\n",
        "- reg_alpha: L1 Regulariztion 적용 \n",
        "- n_estimators\n",
        "\n"
      ],
      "metadata": {
        "id": "1frTKW8oTC7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb_clf=XGBClassifier()"
      ],
      "metadata": {
        "id": "9eEkzok2VEsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**LightGBM**\n",
        "\n",
        "LightGBM의 개요\n",
        "- XGBoost보다 학습에 걸리는 시간이 훨씬 적으며 메모리 사용량도 상대적으로 적음\n",
        "- XGBoost의 단점을 보완하는 방식으로 개발되었으므로 예측 성능은 유사\n",
        "- 유일한 단점은 적은 데이터 세트를 적용할 경우 과적합이 발생하기 쉬움\n",
        "- LightGBM은 일반 GBM 계열의 트리 분할 방법과 다르게 리프 중심 트리 분할 방식 사용\n",
        "  - 트리에 균형을 맞추지 않고 최대 손실 값을 가지는 리프 노드를 지속적으로 분할하면서 트리의 깊이가 깊어지고 비대칭적인 규칙 트리 생성\n",
        "\n",
        "LightGBM의 하이퍼 파라미터\n",
        "- num_iterations: 반복 수행하려는 트리의 개수\n",
        "- learning_rate\n",
        "- max_depth\n",
        "- min_data_in_leaf: 결정 트리의 min_samples_leaf와 동일\n",
        "\n",
        "하이퍼 파라미터 튜닝 방안\n",
        "- num_leaves의 개수를 중심으로 min_child_samples, max_depth를 함께 조정하며 모델의 복잡도를 줄이는 것\n",
        "- num_leaves: 개별 트리가 가질 수 있는 최대 리프 개수, 모델 복잡도를 제어하는 주요 파라미터\n",
        "- min_child_samples: 과적합을 개선하기 위한 파라미터\n",
        "- max_depth: 깊이의 크기를 제한, 과적합을 개선\n"
      ],
      "metadata": {
        "id": "MNUHBR6FVON7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "lgbm_clf=LGBMClassifier()"
      ],
      "metadata": {
        "id": "lFIWzHfZYtHU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}