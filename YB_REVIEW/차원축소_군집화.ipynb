{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPsJszYqmEGfGnyatAI+la",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzaeyun/ESAA22-2/blob/main/YB_REVIEW/%EC%B0%A8%EC%9B%90%EC%B6%95%EC%86%8C_%EA%B5%B0%EC%A7%91%ED%99%94.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 차원 축소\n",
        "\n",
        "###**차원 축소 개요**\n",
        "\n",
        "**차원을 축소해야 하는 이유**\n",
        "- 차원이 증가할수록 데이터 포인트 간의 거리가 기하급수적으로 멀어짐, 회소한 구조를 가지게 됨\n",
        "- 수백 개 이상의 피처로 구성된 데이터 세트의 경우 상대적으로 예측 신뢰도가 떨어짐\n",
        "- 피처가 많을 수록 개별 피처 간의 상관 관계가 높을 가능성이 큼\n",
        "- 선형 회귀와 같은 선형 모델에서는 입력 변수 간의 상관관계가 높을 경우 다중 공선성 문제로 모델의 예측 성능이 저하\n",
        "\n",
        "**차원 축소를 하면?**\n",
        "- 피처 수를 줄이면 더 직관저으로 데이터를 해석\n",
        "- 수십 개 이상의 피처가 있는 데이터의 경우 이를 시각적으로 표현해 데이터의 특성을 파악하기는 불가능\n",
        "- 3차원 이하의 차원 축소를 통해 시각적으로 데이터를 압축해서 표현할 수 있음\n",
        "\n",
        "\n",
        "**차원 축소의 의미**\n",
        "- 피처 선택 feature selection: 특정 피처에 종속성이 강한 불필요한 피처는 아예 제거하고, 데이터의 특징을 잘 나타내는 주요 피처만 선택하는 것\n",
        "- 피처 추출 feature extraction: 기존 피처를 저차원으로 압축해서 추출하는 것\n",
        "  - 새롭게 추출된 중요 특성은 기존 피처를 압축한 것이므로 기존 피처와는 완전히 다른 값\n",
        "  - 기존 피처를 함축적으로 더 잘 설명할 수 있는 다른 공간으로 매핑해 추출하는 것\n",
        "  - 함축적인 특성 추출은 기존 피처가 전혀 인지하기 어려웠던 잠재적인 요소를 추축하는 의미\n",
        "\n",
        "**차원 축소 활용**\n",
        "- 이미지 데이터\n",
        " - 이미지 데이터에서 잠재된 특성을 피처로 도출해 함축적 형태의 이미지 변환과 압축을 수행\n",
        " - 적은 차원이기 때문에 과적합 영향력이 작아져서 예측 성능을 더 끌어올릴 수 있음\n",
        "- 텍스트\n",
        " - 텍스트 문서의 숨겨진 의미를 추출\n",
        " - 문서 내 단어들의 구성에서 숨겨져 있는 시맨틱 의미나 토픽을 잠재 요소로 간주하고 이를 찾아낼 수 있음\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZGe0suDMKwts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**PCA(Principal Component Analysis)**\n",
        "\n",
        "**PCA의 개요**\n",
        "\n",
        "- 대표적인 차원 축소 기법, 여러 변수간의 존재하는 상관관계를 이용해 주성분을 추출해 차우너을 축소하는 기법 \n",
        "- 기존 데이터의 정보 유실이 최소화되는 것이 당연 → PCA는 가장 높은 분산을 가지는 데이터의 축을 찾아 이 축으로 차원을 축소: PCA의 주성분이 됨\n",
        "\n",
        "**PCA 수행 스텝**\n",
        "1. 입력 데이터 세트의 공분산 행령 생성\n",
        "2. 공분산 행렬의 고유 벡터와 고유값을 계산\n",
        "3. 고유값이 큰 순서로 K개 만큼 고유벡터 추출\n",
        "4. 고유값이 큰 순으로 추출된 고유벡터를 이용해 새롭게 입력한 데이터를 변환\n"
      ],
      "metadata": {
        "id": "5aoRl7wzWt3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca=PCA(n_components=6)"
      ],
      "metadata": {
        "id": "VsaR6-NKWpJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###**LDA(Linear Discriminant Analysis)**\n",
        "\n",
        "**LDA 개요**\n",
        "\n",
        "- 입력 데이터 세트를 저차원 공간에 투영하여 차원 축소\n",
        "- 지도학습의 분류에서 사용하기 쉽도록 개별 클래스를 분별할 수 있는 기준을 최대한 유지하면서 차원을 축소\n",
        "- 클래스 분리를 최대화하는 축을 찾기 위해 클래스 간의 분산과 클래스 내부 분산의 비율을 최대화하는 방식으로 차원을 축소\n",
        "\n",
        "**LDA 수행 스텝**\n",
        "1. 클래스 내부와 클래스 간 분산 행렬을 구하고 이는 입력 데이터의 결정 값 클래스 별로 개별 피처의 평균 벡터를 기반으로 구함\n",
        "2. 클래스 내부 분산 행렬을 클래스 간 분산 행렬을 Sb라고 하면 두 행렬의 고유 벡터로 분해\n",
        "3. 고유 값이 큰 순서로 K개 추출\n",
        "4. 고유 값이 큰 순서로 추출된 고유벡터를 이용해 새롭게 입력 데이터 변환\n"
      ],
      "metadata": {
        "id": "cM2RzAykWmKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda=LinearDiscriminantAnalysis(n_components=2)"
      ],
      "metadata": {
        "id": "2CCh4wgzWh8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**SVD(Singular Value Decomposition)**\n",
        "\n",
        "**SVD 개요**\n",
        "- PCA와 유사한 행렬 분해 기법 이용하면서 행과 열의 크기가 다른 행렬에도 적용\n",
        "- SVD는 특이값 분해로 불리며, U와 V에 속한 벡터는 특이벡터이고, 모든 특이 벡터는 서로 직교\n",
        "- 시그마는 대각 행렬로 대각에 위치한 값만 0이 아니고 나머지는 모두 0\n",
        "  - 일반적으로 시그마의 특이값이 0인 부분도 제거하고 이에 대응되는 U와 V 원소도 함께 제거하여 차원을 줄인 형태로 SVD 적용\n"
      ],
      "metadata": {
        "id": "8gfhQlXXWRM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.linalg import svd\n",
        "\n",
        "#U, Sigma, Vt=svd(a)"
      ],
      "metadata": {
        "id": "xsc8aLfkWKpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**NMF(Non-Negative Matrix Factorization)**\n",
        "\n",
        "**NMF 개요**\n",
        "- 원본 행렬 내의 모든 원소 값이 모두 양수라는 게 보장되면 두 개의 기반 양수 행렬로 분해될 수 있는 기법을 지칭\n",
        "- n * p 행렬이면 n * a X a * p 로 분해\n",
        "- SVD와 유사하게 차원 축소를 통해 잠재 요소 도출로 이미지 변환 및 압축, 텍스트의 토픽 도출 등의 영역에서 사용"
      ],
      "metadata": {
        "id": "7OIDwzg4V_XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "nmf=NMF(n_components=2)"
      ],
      "metadata": {
        "id": "xEtPSEc4V9jm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> 군집화\n",
        "\n",
        "###**K-평균 알고리즘 이해**\n",
        "\n",
        "**K-평균**\n",
        "- 군집화에서 가장 일반적으로 사용되는 알고리즘\n",
        "- 군집 중심점이라는 특정한 임의의 지점을 선택해 해당 중신에서 가장 가까운 포이트들을 선택하는 군집화 기법\n",
        "\n",
        "**K-평균의 장점**\n",
        "- 알고리즘이 쉽고 간결함\n",
        "\n",
        "**K-평균의 단점**\n",
        "- 거리 기반 알고리즘으로 속성의 개수가 매우 많을 경우 군집화 정확도가 떨어짐\n",
        "- 몇 개의 군집을 선택해야 할지 가이드가 어려움\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EMpRIroPV3rL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsSjOm0RFEaR"
      },
      "outputs": [],
      "source": [
        "class sklearn.cluster.KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001,\n",
        "                             precompute_distances='auto', verbose=0, random_state=None,\n",
        "                             copy_x=True, n_jobs=1, algorithm='auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**K-평균의 주요 파라미터**\n",
        "- n_clusters: 군집화할 개수\n",
        "- init: 초기에 군집 중심점의 좌표를 설정할 방식 ( 보통 k-means++ 방식으로 설정 )\n",
        "- max_iter: 최대 반복 횟수: 모든 중심점 이동이 없으면 종료\n",
        "\n",
        "**K-평균의 주요 속성**\n",
        "- labels_: 각 데이터 포인트가 속한 군집 중심점 레이블\n",
        "- cluster_centers_: 각 군집 중심점 좌표\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TVLTBXKSYai0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **군집 평가(Cluster Evaluation)**\n",
        "\n",
        "**실루엣 분석(silhouette analysis)**\n",
        "\n",
        "**실루엣 분석의 개요**\n",
        "- 군집 간의 거리가 얼마나 효율적으로 분리되어 있는지를 나타냄\n",
        "- 효율적으로 잘 분리되었다는 것은 다른 군집과의 거리는 떨어져있고, 동ㅇ리 군집끼리의 데이터는 서로 가깝게 잘 뭉쳐 있다는 의미\n",
        "\n",
        "**실루엣 계수**\n",
        "- 실루엣 분석은 실루엣 계수를 기반으로 하며 실루엣 계수는 개별 데이터가 가지는 군집화 지표\n",
        "- 같은 군집 내의 데이터와 얼마나 가깝게 군집화 되어있으며, 다른 군집과는 얼마나 멀리 분리 되어있는지를 나타내는 지표\n",
        "- -1 에서 1 사이의 값으로\n",
        "  - 1로 가까워질수록 근처의 군집과 더 멀리\n",
        "  - 0에 가까울수록 근처 군집과 가까워진다는 것\n",
        "  - 음수 - 값은 아예 다른 군집에 데이터 포인트가 할당되었음\n"
      ],
      "metadata": {
        "id": "0pdm9iKfY49Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "\n",
        "#score_samples=silhouette_samples(iris.data, irisDF['cluster'])\n",
        "#average_score=silhouette_score(iris.data, irisDF['cluster'])"
      ],
      "metadata": {
        "id": "f3TRtRpGZuY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**좋은 군집화의 조건**\n",
        "- 전체 실루엣 계수의 평균값이 1에 가까울수록 조흥ㅁ\n",
        "- 하지만 평균값뿐만 아니라 개별 군집의 평균값의 편차가 크지 않아야함\n",
        "  - 개별 군집의 평균값도 골고루 높아야 좋은 것\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "amnH7mh5Z1WU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**평균 이동(Mean Shift)**\n",
        "\n",
        "**평균 이동의 개요**\n",
        "- K-평균과 유사하게 중심을 군집의 중심으로 지속적으로 움직이면서 군집화 수행\n",
        "- 중심을 데이터가 모여 있는 밀도가 가장 높은고승로 이동\n",
        "- 확률 밀도 함수가 피크인 점이 가장 집중적으로 데이터가 모여있기 때문에 군집 중심점으로 선정\n",
        "- 주어진 모델의 확률 밀도 함수를 찾기 위해 KDE 사용\n",
        "  - KDE(Kernel Density Estimation)\n",
        "  - 커널 함수를 통해 어떤 변수의 확률 밀도 함수를 추정하는 대표적인 방법"
      ],
      "metadata": {
        "id": "mF7Aq_5MaFEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import MeanShift \n",
        "\n",
        "meanshift=MeanShift(bandwidth=1)\n",
        "#cluster_labels=meanshift.fit_predict(X)"
      ],
      "metadata": {
        "id": "nTa4eWC_amTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**GMM(Gaussian Mixture Model**\n",
        "\n",
        "**GMM의 개요**\n",
        "- 군집화를 적용하고자 하는 데이터가 여러 개의 가우시안 분포를 가진 데이터 집합들이 섞여서 생성된 것이라는 가정하에 군집화 수행\n",
        "- 정규분포로도 알려진 가우시안 분포는 좌우 대칭형의 종 형태\n",
        "- 서로 다른 정규 분포 형태를 가진 여러 가지 확률 분포 곡선으로 구성되어 있으며, 이를 추출하여 개별 데이터가 이 중 어떤 정규 분포에 속하는지 결정\n",
        "\n"
      ],
      "metadata": {
        "id": "zfY5f-1GatiB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "\n",
        "#gmm=GaussianMixture(n_components=3, random_state=0).fit(iris.data)\n",
        "#gmm_cluster_labels=gmm.predict(iris.data)"
      ],
      "metadata": {
        "id": "jCCBaWtZbJh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**DBSCAN**\n",
        "\n",
        "**DBSCAN의 개요**\n",
        "- 밀도 기반의 군집화로 간단하고 직관적인 알고리즘임에도 효과적인 군집화 가능\n",
        "- 내부의 원과 외부의 원 모양의 형태의 분포를 가진 데이터 세트를 군집화 할 때 좋은 수행 결과를 보임\n",
        "\n"
      ],
      "metadata": {
        "id": "rc8QLSCNbOL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#bscan=DBSCAN(eps=0.2, min_samples=10, metric='euclidean')\n",
        "#dbscan_labels=dbscan.fit_predict(X)"
      ],
      "metadata": {
        "id": "okK5O5ldblmY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}